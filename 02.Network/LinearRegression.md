# 线性回归

回归：寻找输入变量与输出变量间的对应关系，常通过类似于对已有数据集进行拟合的方法进行。但考虑到模型稳定性和泛化性，并非对训练数据集拟合误差越小越好，需要加入正则化过程。

应用：股市预测、自动驾驶、商品推荐。

## 学习步骤

### 模型假设

#### 线性模型

$$ y = Wx + b $$

Y为输出向量，X为输入向量，W与B为线性映射参数。学习过程即是寻找最优参数，使得输入经此映射后尽可能接近输出。

线性模型的特点：

+ 模型简单，计算方便，可以根据系数给出每个变量的理解和解释。
+ 即使复杂的映射在局部也可以很好地近似为线性映射。所以，线性模型的适用范围较广。也因此，线性模型主要适用于输入输出间的映射相对“光滑”的情况，即当输入变化不大时，输出变化也不大。

拟合效果时不好常见改进方案：

+ 存在对输入影响较大的特征输入未被考虑。可通过添加输入特征优化。
+ 映射的线性程度不高。可通过对原先特征进行非线性化处理，例如添加lnX、X^n、e^X等方式优化。

### 模型评估

设定对模型反应映射关系能力的判据。
损失函数用来评价模型的预测值与真实值的不一致程度，它是一个非负实值函数。常见型式：

+ 误差绝对值均值，$ L(f)=\frac{1}{N}\sum |e| $ （N为数据集样本数，e为预测误差）
+ 误差平方均值，$ L(f)=\frac{1}{N}\sum e^2 $

### 模型参数最优化

使模型参数达到最优，损失函数最小。

#### 梯度下降

由于给出的训练数据集固定，损失函数只与模型参数有关。所以可对损失函数在参数上的梯度来更新参数。
$$ P_{i+1} = P_{i} - \eta \frac{\partial L}{\partial P}(P_{i}) $$

例如对于线性模型和取误差平方均值的损失函数来说，

$$ P=[W,b] $$
$$ L(P)=\frac{1}{N}\sum (Wx+b-\hat y)^T(Wx+b-\hat y) $$
$$ \frac{\partial L}{\partial P}(P_{i})=\frac{1}{N} \sum 2(W_ix+b_i-\hat y)[x^T,1] $$

#### 最小二乘法

对于线性方程组 $ Ax=b $，当 $ x_0=(A^T A)^{-1} A^T b $ 时， $ (A x_0)^Tb $ 最小。

对于多输入，单输出的线性模型 $ y=w^Tx+b $，当
$$ (w,b)=(\hat X_1^T \hat X_1)^{-1} \hat X_1^T \hat y $$
时误差平方均值最小，其中$\hat X_1=[\hat X|1]$，$ \hat X,\hat y $每行分别对应一组输入输出。

### 正则化

更多特征会更加拟合训练数据集。但训练数据集存在局限性，不能代表所有可能遇到的数据。权重参数过多可能会导致某些特征权值过高，导致过度拟合，所以在损失函数中加入与参数大小有关的项。
$$ L'(P) = L(P)+ \lambda ||P||^2 $$

### 模型检验

在求解完最优参数后。=，还需要对未被加入过训练数据集的数据进行测试，以检验模型在其他情况下的预测能力。