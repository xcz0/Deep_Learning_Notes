# 动量梯度下降法

动量梯度下降法（​Momentum），这也是另外一个，有可能可以对抗 Saddle Point，或 Local Minima 的技术。

## Small Gradient

<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315165829425.png" alt="image-20210315165829425" style="zoom:67%;" />

可以想像成在物理的世界里，假设 Error Surface 就是真正的斜坡，而参数是一个球，把球从斜坡上滚下来。如果是 Gradient Descent，它走到 Local Minima 或 Saddle Point 就停住了。但是在物理的世界里，一个球如果从高处滚下来，就算滚到 Saddle Point，如果有**惯性**，它还是会继续往右走，甚至它走到一个 Local Minima，如果动量够大的话，它还是会继续往右走，甚至翻过这个小坡然后继续往右走。所以在物理的世界里，一个球从高处滚下来的时候，它并不会被 Saddle Point 或 Local Minima卡住，有没有办法运用这样子的概念，到 Gradient Descent 里面呢，这个就 Momentum 这个技术

## 一般梯度下降（Vanilla Gradient Descent）

<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315170131552.png" alt="image-20210315170131552" style="zoom:67%;" />

​一般的 Gradient Descent 是说，有一个初始的参数叫做 $θ^0$，计算一下 Gradient，然后计算完这个 Gradient 以后呢，往 Gradient 的反方向去 Update 参数
$$ θ^1 = θ^0 - {\eta}g^0 $$
​到了新的参数以后，再计算一次 Gradient，再往 Gradient 的反方向，再 Update 一次参数，到了新的位置以后再计算一次 Gradient，再往 Gradient 的反方向去 Update 参数，这个 Process 就一直这样子下去

## 梯度下降 + 动量

​加上 Momentum 以后，每一次在移动的参数的时候，不只往 Gradient 的反方向来移动参数，是 **Gradient 的反方向，和上前一步移动的方向，两者加起来的结果，调整去到的参数**。



​<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315173052976.png" alt="image-20210315173052976" style="zoom: 67%;" />

​接下来就反覆进行同样的过程，在这个位置计算出 Gradient，但不是只根据 Gradient 反方向走，看前一步怎麼走，前一步走这个方向，走这个蓝色虚线的方向，把蓝色的虚线加红色的虚线，前一步指示的方向跟 Gradient 指示的方向，当做下一步要移动的方向

​<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315192909326.png" alt="image-20210315192909326" style="zoom:67%;" />

​每一步的移动，都用 m 来表示，那这个 m 其实可以写成之前所有算出来的，Gradient 的 Weighted Sum.从右边的这个式子，其实就可以轻易的看出来
$$
m^0 = 0\\
m^1 = -{\eta}g^0\\
m^2 = -{\lambda}{\eta}g^0-{\eta}g^1\\
...
$$
m0 把它设為 0，m1 是 m0 减掉 g0，m0 為 0，所以 m1 就是 g0 乘上负的 η，m2 是 λ 乘上 m1，λ 就是另外一个参数，就好像 η 是 Learning Rate 要调，λ 是另外一个参数，这个也是需要调的，m2 等於 λ 乘上 m1，减掉 η 乘上 g1，然后 m1 在哪里呢，m1 在这边，你把 m1 代进来，就知道说 m2，等於负的 λ 乘上 η 乘以 g0，减掉 η 乘上 g1，它是 g0 跟 g1 的 Weighted Sum

​以此类推，所以你会发现说，现在这个加上 Momentum 以后，一个解读是 Momentum 是，Gradient 的负反方向加上前一次移动的方向，那但另外一个解读方式是，所谓的 Momentum，当加上 Momentum 的时候， Update 的方向，不是只考虑现在的 Gradient，而是考虑过去所有 Gradient 的总合.

​有一个更简单的例子，希望帮助你了解 Momentum 

<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315193547074.png" alt="image-20210315193547074" style="zoom:50%;" />

​那从这个地方开始 Update 参数，根据 Gradient 的方向告诉，应该往右 Update 参数，那现在没有前一次 Update 的方向，所以就完全按照 Gradient 给的指示，往右移动参数，好 那的参数，就往右移动了一点到这个地方

<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315193633595.png" alt="image-20210315193633595" style="zoom:50%;" />

Gradient 变得很小，告诉往右移动，但是只有往右移动一点点，但前一步是往右移动的，把前一步的方向用虚线来表示，放在这个地方，把之前 Gradient 告诉要走的方向，跟前一步移动的方向加起来，得到往右走的方向，那再往右走 走到一个 Local Minima，照理说走到 Local Minima，一般 Gradient Descent 就无法向前走了，因為已经没有这个 Gradient 的方向，那走到 Saddle Point 也一样，没有 Gradient 的方向已经无法向前走了

<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315193703079.png" alt="image-20210315193703079" style="zoom:50%;" />

​但没有关係，如果有 Momentum 的话，你还是有办法继续走下去，因為 Momentum 不是只看 Gradient，Gradient 就算是 0，你还有前一步的方向，前一步的方向告诉向右走，就继续向右走，甚至你走到这种地方，Gradient 告诉你应该要往左走了，但是假设你前一步的影响力，比 Gradient 要大的话，你还是有可能继续往右走，甚至翻过一个小丘，搞不好就可以走到更好 Local Minima，这个就是 Momentum 有可能带来的好处
